%\documentclass[a4paper]{article}
\documentclass[10pt]{scrartcl}
\setkomafont{disposition}{\bfseries}
\usepackage[sc]{mathpazo}
\linespread{1.05}         % Palladio needs more leading (space between lines)
\usepackage[T1]{fontenc}
%\usepackage{lmodern}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{arrows.meta}
\usetikzlibrary{calc}
\usetikzlibrary{decorations.markings}
\usetikzlibrary{hobby}
\graphicspath{ {images/} }
\usepackage{physics}
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage[export]{adjustbox}
\usepackage[framemethod=tikz]{mdframed}
\usepackage[colorinlistoftodos]{todonotes}

\newmdenv[
	%  topline=false,
	%  bottomline=false,
	%  skipabove=\topsep,
	%  skipbelow=\topsep
]{siderules}

\hypersetup{
	colorlinks=true,
	linktoc=all,
	linkcolor=blue,
}

\newcommand\todoin[2][]{\todo[inline, caption={2do}, #1]{
\begin{minipage}{\textwidth-4pt}TODO: #2\end{minipage}}}

	\numberwithin{equation}{subsection}
	\theoremstyle{definition}
	\newtheorem{definition}{Definition}[section]

	\newtheorem{theorem}{Theorem}[section]

	\theoremstyle{remark}
	\newtheorem*{remark}{Remark}
	\newtheorem*{example}{Example}
	\newenvironment{definitionSR}
	{
		\begin{siderules}
			\begin{definition}
			}
			{
			\end{definition}
		\end{siderules}
	}

	\newenvironment{theoremSR}
	{
		\begin{siderules}
			\begin{theorem}
			}
			{
			\end{theorem}
		\end{siderules}
	}

	\DeclareMathOperator{\sign}{sign}
	\DeclareMathOperator{\Log}{Log}
	\newcommand{\Reals}{\mathbb{R}}
	\newcommand{\Complex}{\mathbb{C}}
	\newcommand{\Integers}{\mathbb{Z}}
	\newcommand{\ReP}[1]{
		\mathrm{Re}\pqty{#1}
	}
	\newcommand{\ImP}[1]{
		\mathrm{Im}\pqty{#1}
	}
	\newcommand{\ve}[1]{
		\mathbf{#1}
	}
	\newcommand{\cvb}[1]{
		[\mathbf{#1}]
	}
	\newcommand{\inv}[1]{
		{#1}^{-1}
	}
	\newcommand{\pdvc}[3]{
		\left(\pdv{#1}{#2}\right)_{#3}
	}
	\newcommand*{\conjugate}[1]{\overline{#1}}
	\newcommand*{\conj}[1]{{#1}^*}
	\DeclareMathOperator{\Arg}{Arg}
	%\newcommand{\ip}[2]{
	%	\langle {#1} | {#2} \rangle
	%}
	%\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
	\title{Notes for Math 121A}

	\author{Harrison Bachrach}

	\date{Spring 2015}


	\begin{document}
	\tikzset{%
		->-/.style={%
			decoration={markings, mark=at position #1 with {\arrow{>}}}, postaction={decorate}
		},
		light fill/.style={%
			fill=black!10%
		}%
	}

	\maketitle

	\begin{abstract}
		Notes for Math 121A based on lectures given by Dr.\ Nikhil Srivastava in Spring
		2015 at UC Berkeley in Berkeley, California.  The following notes assume basic
		knowledge of series, multivariable calculus and linear algebra; however, the
		first few sections are dedicated to the brief review of those topics.

		For those of you scientists and engineers who have taken up to linear algebra
		\& multivariable calculus \emph{but no further}, I entreat you to continue
		reading. After the review, Sections~\ref{ComplexAnalysis}
		and~\ref{HarmonicAnalysis} (on Complex Analysis and Harmonic Analysis
		respectively) demonstrate the humbling power and beauty of higher-level
		mathematics.  While the topics are perhaps not expressed here with the rigor
		that would satisfy a mathematician, I hope that they can offer a stimulating
		preview of ``upper division'' mathematics.

		Sections suffixed with an asterix, i.e.\ the ``*'' symbol, are generally
		``optional'' in that they don't \emph{necessarily} have applicability to the
		physical sciences. However, they are often quite beautiful and/or significant
		within pure mathematics.
	\end{abstract}

	\tableofcontents
	\section{Notation}
	\subsection{Screen \& Print Notation}
	Many symbols of set theory and boolean logic are useful beyond their original applications.
	In this section, I will be explaining the meaning of those symbols which are used here which you may not have encountered previously.
	\begin{itemize}
		\item A \textit{set} of things is denoted by curly braces. E.g.\ The set of
			natural numbers $\mathbb{N} = \Bqty{0, 1, 2, 3, \ldots}$. \\
		\item The symbol $\in$ means ``in'' or ``is an element of''. For example, the
			statement $2 \in \mathbb{N}$ is true, while $ -3 \in \mathbb{N}$ is
			not.
		\item The symbol $\forall$ means ``for all''. For example, $\forall x \in
			\mathbb{N} ~ x > -1$ is a true statement. \\
\item The symbol $\Rightarrow$ means ``implies'', that is, ``$p \Rightarrow
	r$'' is to be read as ``if $p$ is true, then $q$ is also true''.
	Note that it states nothing about the status of $p$ if we are given the status of $q$,
	nor does it tell us the status of $q$ if $p$ is false. \\
\item The symbol $\iff$ means ``if and only if''\footnote{``if and only if'' is
	often written as ``iff''}, that is, ``$p \iff r$'' is to be read as
	``if and only if $p$ is true, then $q$ is also true'' \emph{and} ``if
	and only if $q$ is true, then $p$ is also true.'' Intuitively, this
	logically links $p$ and $q$.
\item The symbol $\wedge$ means ``and''. A statement ``$P(x) \wedge Q(x)$'' is
	only true if \emph{both} $P(x)$ and $Q(x)$ are true statements. 
	E.g., $(5>3 \wedge 2 \neq 0)$ is true, while $(3>6 \wedge 4 = 4)$ is false. \\
\item We can build sets by using \textit{set builder notation}.
	The notation works as follows: a set $S$ of elements that satisfy a condition
	is denoted 
	\[
		S = \Bqty{\text{Element}~|~\text{Condition on element}}.
	\]
	For example, the set of natural numbers can be expressed as 
	\[
		\mathbb{N} = \Bqty{x~|~x \in \mathbb{Z} \wedge x \geq 0}
	\]
	where $\mathbb{Z}$ is the set of integers. \\
\item A very useful notation for talking about intervals on the real line is \textit{interval notation}. It works as follows:
	\begin{align*}
		(a,b) &= \Bqty{x~|~x \in \Reals \wedge a<x<b} \\
		[a,b) &= \Bqty{x~|~x \in \Reals \wedge a\leq x<b} \\
		(a,b] &= \Bqty{x~|~x \in \Reals \wedge a<x\leq b} \\
		[a,b] &= \Bqty{x~|~x \in \Reals \wedge a\leq x\leq b} \\
	\end{align*} \\


\end{itemize}
\subsection{Handwritten Notation}
%%%% SERIES

\section{Series}
\begin{definitionSR}
	A \textit{series} is the sum of the items in a sequence.
\end{definitionSR}


\emph{Finite} sums are always well defined, while \emph{infinite} sums must meet certain criteria to be well defined. A finite sum up to term $a_N$ would be denoted
\[
	\sum_{n=0}^N a_n = S_N
\]
While an infinite sum, that is the limit of a finite sum (assuming the limit exists) is denoted as
\[
	\lim_{N \to \infty} S_N = \lim_{N \to \infty} \sum_{n=0}^N a_n = \sum_{n=0}^\infty a_n = S
\]
If the above limit exists, the series is \textit{convergent}, while if it doesn't, the series is \textit{divergent}.

\begin{definitionSR}
	A \textit{geometric series} is one where terms are related by a common ratio $r$. The general geometric series up to order $N$ could be expressed as
	\[
		S_N = a + ar + ar^2 + \cdots + ar^N
	\]
	The formula for a finite geometric series is
	\begin{equation}
		S_N = \frac{a(1-r^N)}{1-r}
	\end{equation}
	and the formula for an infinite geometric series (assuming the series exists, i.e.\ assuming $|r|<1$) is simply (by taking the limit $N \to \infty$),
	\begin{equation}
		S = \frac{a}{1-r}
	\end{equation}
	where in both cases, a and r represent the first term and common ratio, respectively.\\
\end{definitionSR}


\begin{definitionSR}
	The \textit{remainder} $R_N$ is the difference between the infinite series $S$ and the ``partial sum'' $S_N$, that is,
	\[
		R_N = \sum_{k=N+1}^\infty a_k = S-S_N
	\]
\end{definitionSR}

Series are a way of breaking something difficult into a sum of easy to handle
terms.\todo{Expand this in relation to rest of course}

\subsection{Tests for Convergence}

\begin{definitionSR}[Comparing Magnitudes]
	\[
		f \ll g \iff \lim_{n \to \infty } \frac{f(n)}{g(n)} = 0
	\]
\end{definitionSR}

Do note some general relative magnitudes of growth (assume $n, k \in \Integers$):
\begin{equation}
	\log n \ll n \ll n^k \ll \ll 2^n \ll n!
\end{equation}

\subsubsection{Preliminary Test}
If $\lim_{n \to \infty} a_n \neq 0$, then $\sum_n^\infty a_n$ diverges.

\subsubsection{Integral Test}
If $a_n$ is non-negative and non-increasing, then
\[
	\sum_{n=1}^\infty a_n \text{\, converging} \iff \int_b^\infty a(x)\,dx \text{\, converging}
\]
where $a(x)$ is the continuation of $a_n$ for the real variable $x$; e.g.\ if $a_n = 1/n$, then $a(x) = 1/x$, where $x \in \mathbb{R}$.
\subsubsection{Comparison Principle}
Suppose $a_n$ and $b_n$ are sequences with $0 \leq a_n \leq b_n$ for all $n$.
\begin{align*}
\sum b_n \text{\, converges}  &\Rightarrow \sum a_n \text{\, converges.} \\
\sum a_n \text{\, diverges}  &\Rightarrow \sum b_n \text{\, diverges.}
\end{align*}

\subsubsection{Special Comparison Principle}
If $a_n$, $b_n$ are non-negative sequences and $\lim_{n \to \infty} \frac{a_n}{b_n} < \infty$ (that is to say, the limit is finite), then
\[
\sum b_n \text{\, converges}  \Rightarrow \sum a_n \text{\, converges.} 
\]

\subsubsection{Ratio Test}
Defining
\begin{center}
	\begin{tabular}{cc}
		$ \rho_n = \displaystyle \left|\frac{a_{n+1}}{a_n}\right| $, & $ \rho = \displaystyle \lim_{n \to \infty} \rho_n = \displaystyle \lim_{n \to \infty} \displaystyle \left|\frac{a_{n+1}}{a_n}\right| $
	\end{tabular}
\end{center}
then
\begin{align*}
\rho < 1 &\Rightarrow \text{the series converges.} \\
\rho > 1 &\Rightarrow \text{the series diverges.} \\ 
\rho = 1 &\Rightarrow \text{the test is inconclusive.}
\end{align*}

\subsubsection{Alternating Series Test}
If $a_n$ is an alternating series (i.e., $\sign(a_{n+1}) = -\sign(a_n)$), $|a_{n+1}| \leq |a_n|$, \emph{and} $\lim_{n \to \infty} = 0$, then the series converges.

Note: if $\sum |a_n|$ converges, the series is \textit{absolutely convergent}.

\subsection{Power Series}
\begin{definitionSR}
	A \textit{power series} is a function expressed as an infinite sum of monomials. That is, a function $f(x)$ can be expanded in a power series if it can be expressed as
	\[
		f(x) = \sum_{n=0}^\infty a_n x^n
	\]
	for some $\{a_n\}$.
\end{definitionSR}

\begin{theoremSR}
	A power series expansion of $f(x)$ converges on only 3 types of intervals.
	\begin{enumerate}
		\item It converges everywhere (that is, $\forall x$)
		\item It converges for $x=0$ only
		\item It converges when $|x|<R$ and diverges when $|x|>R$, where $R$ is the radius of convergence. (The points $x=\pm R$ must be checked explicitly and are not, in general, symmetric.)
	\end{enumerate}
\end{theoremSR}

\subsubsection{Taylor Series}

\begin{definitionSR}
	The power series expansion of an analytic function about a number $a$ is known as a \textit{Taylor series}. When $a=0$, this is known as a \textit{Maclaurin series}. If $f(x)$ is analytic,
	\[
		f(x-a) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \cdots + \frac{f^{(n)}}{n!}(x-a)^n + \cdots
	\]
	Note: the radius of convergence for Taylor series depends upon $a$.
\end{definitionSR}

Here are some common Maclaurin series, with their interval of convergence in parentheses:\\
\newline

\begin{adjustbox}{center}
	\begin{tabular}{rclr}

		$\sin(x) \quad = $&$  \displaystyle \sum_{n=0}^\infty \frac{(-1)^n x^{2n+1}}{(2n+1)!} $&$ = \quad \displaystyle x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots$ & $ (x \in \Reals) $\\

		\midrule

		$\cos(x) \quad = $&$   \displaystyle  \sum_{n=0}^\infty \frac{(-1)^n x^{2n}}{(2n)!} $&$ = \quad \displaystyle 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \cdots$ & $ (x \in \Reals) $\\


		\midrule

		$e^x \quad = $&$ \displaystyle \quad \sum_{n=0}^\infty \frac{x^n}{n!} $&$ = \quad \displaystyle 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots$ & $ (x \in \Reals) $\\

		\midrule

		$\log(1+x) \quad =  $&$  \displaystyle \quad \sum_{n=1}^\infty \frac{(-1)^{n+1}x^n}{n} $&$ = \quad \displaystyle x - \frac{x^2}{2} + \frac{x^3}{3} - \frac{x^4}{4} \cdots$ & $ (x \in (-1,1]\,) $\\

		\midrule

		$(1+x)^p \quad = $&$ \displaystyle \quad \sum_{n=0}^\infty \binom{p}{n} x^n $&$ = \quad \displaystyle 1 + px + \frac{p(p-1)}{2!}x^2  + \frac{p(p-1)(p-2)}{3!}x^3 + \cdots$ & $ (x \in (-1,1)\,) $\\

	\end{tabular}
\end{adjustbox}

\subsubsection{Asymptotic Notation}
How do we write higher order terms we are not concerned with so that we can keep track of them? With ``Little-oh'' and ``Big-Oh'' notation!

\begin{definitionSR}[Little-oh notation]
	Given continuous functions $f(x)$ and $g(x)$, we say that $f(x) = o(g(x))$ as $x \to a$ if
	\[
		\lim_{x \to a} \frac{f(x)}{g(x)} = 0
	\]
	e.g.\ $x^5 = o(x)$ as $x \to 0$, $x^4 = o(x^5)$ as $x \to \infty$, etc.
\end{definitionSR}

\begin{definitionSR}[Big-Oh notation]
	Given continuous functions $f(x)$ and $g(x)$, we say that $f(x) = (g(x))$ as $x \to a$ if
	\[
		\lim_{x \to a} \left|\frac{f(x)}{g(x)}\right| < \infty
	\]
	e.g.\ $x^2 = O(x^2)$ as $x \to 0$, $2\sin x = O(1)$ as $x \to \infty$, etc.
\end{definitionSR}
\todo{Condsider changing examples}

The rules for manipulating the notation is as follows:

\begin{enumerate}
	\item If $c \in \Reals$ and $f(x) = o(g(x))$, then $cf(x)=o(g(x))$.

	\item If $f_1(x) = o(g_1(x))$ and $f_2(x) = o(g_2(x))$, then $f_1(x)f_2(x) = o(g_1(x)g_2(x))$.

	\item If $f(x) = o(g(x))$, then $x\cdot f(x) = o(x\cdot g(x))$.

	\item If $\displaystyle \lim_{x \to 0} g(x) = 0$, then $\displaystyle \frac{1}{1+g(x)} = 1 - g(x) + o(g(x))$.

	\item $o(f(x)+g(x)) = o(f(x)) + o(g(x))$

	\item $o(o(f(x))) = o(f(x))$

\end{enumerate}

All of the above apply to ``Big-Oh'' notation as well.

Both notations are commonly used in expressing series so that one can keep track of the order of an approximation. For example, if we expand $e^x$ about $x=0$ but are only concerned with second-order terms and below, we can write it as 
\[
	e^x = 1+x+\frac{x^2}{2!}+o(x^2) = 1+x+\frac{x^2}{2!} + O(x^3)
\]

\subsection{Error of Series Approximations}
In general, the error for a finite \emph{power} series can be expressed as 
\begin{align}
	R_N(x) &= f(x) - \left( f(x) + (x-a)f'(a) + \cdots + (x-a)^N \frac{f^{(N)}(a)}{N!}\right) \\
	&= \frac{(x-a)^{N+1}f^{(N+1)}(c)}{(N+1)!}
\end{align}

for some $c \in [a,x]$. This is known as \textit{Taylor's Theorem}. However, it
is not of much practical use, as we do not have a formula for
$c$.
\todo{However, useful for error bounds}

In the case where the power series coefficients are \emph{decreasing} and \emph{non-negative}, on the interval $x \in (-1,1)$ we can use the formula
\begin{equation}
	R_N(x) = \frac{a_{N+1}x^N}{1-|x|}
\end{equation}

In the case where the (not necessarily power) series is \emph{alternating}, and absolutely decreasing (that is, $|a_{n+1}|<|a_n|$ for all $n$) and $\lim_{n \to \infty} a_n = 0$, then 
\begin{equation}
	|R_N| \leq |a_{N+1}|.
\end{equation}


%%%%% LINEAR ALGEBRA

\section{Linear Algebra}

\subsection{Notation}

\begin{itemize}
	\item A \textit{vector} in a vector space is denoted as $\ve{x}$.
	\item The \textit{coordinate vector} of $\ve{x}$ with respect to a basis $\mathcal{B}$ is denoted as $\cvb{x}_\mathcal{B}$. If there is no subscript, e.g.\ $\cvb{x}$, the basis is understood to be the \textit{standard basis}.
	\item The \textit{standard basis elements} are denoted $\ve{e}_1, \ve{e}_2, \ldots \ve{e}_n$.
\end{itemize}

\subsection{Inner Product Spaces}

\begin{definitionSR}
	An \textit{inner product} is a billinear map that takes two vectors of a vector
	space and returns a scalar of a field $F$ (where in these notes, $F=\Reals$ or
	$F=\Complex$).
	In more symbolic notation, we have $\langle \cdot , \cdot \rangle : V \times V
	\to F$. A billinear map qualifies as an inner product if all of the following
	conditions are
	met\footnote{Where $z^*$ is the complex conjugate of $z$, also written as
	$\bar{z}$. For more information, see Section~\ref{ComplexNumbers}.}:
	\begin{enumerate}
		\item $\braket{\ve{x}}{\ve{x}} \geq 0$ and $\braket{\ve{x}}{\ve{x}} = 0 \iff \ve{x} = \ve{0}$
		\item $\braket{a_1\ve{x}_1 + a_2\ve{x}_2}{\ve{y}} = a_1^*\braket{\ve{x}_1}{\ve{y}} + a_2^*\braket{\ve{x}_2}{\ve{y}}$
		\item $\braket{\ve{x}}{b_1\ve{y}_1 + b_2\ve{y}_2} = b_1\braket{\ve{x}}{\ve{y}_1} + b_2\braket{\ve{x}}{\ve{y}_2}$
		\item $\braket{\ve{x}}{\ve{y}} = \braket{\ve{y}}{\ve{x}}^*$
	\end{enumerate}

\end{definitionSR}
\begin{definitionSR}
	The \textit{norm} of a vector $\ve{x}$ is $\norm{\ve{x}} = \sqrt{\braket{\ve{x}}{\ve{x}}}$.
\end{definitionSR}

From these we get a few theorems, which we will state here without proof.

First we have the \textit{Cauchy-Schwarz Inequality}:
\begin{equation}
	\braket{\ve{p}}{\ve{q}} \leq \norm{\ve{p}}\cdot\norm{\ve{q}}.
\end{equation}
Next we have the \textit{Triangle Inequality}:
\begin{equation}
	\norm{\ve{p} + \ve{q}} \leq \norm{\ve{p}} + \norm{\ve{q}}.
\end{equation}
Lastly we have the \textit{Pythagorean theorem}:
\begin{equation}
	\braket{\ve{p}}{\ve{q}} = 0 \iff \norm{\ve{p}}^2 + \norm{\ve{q}}^2 =
	\norm{\ve{p} + \ve{q}}^2.
\end{equation}

For our purposes, there are only four important inner products to concern
ourselves with. When we're dealing with vectors in $\Reals^n$, we will use the
familiar \textit{dot product}; if $\vb{x}, \vb{y} \in \Reals^n$, then 
\begin{equation}
	\braket{\vb{x}}{\vb{y}} = \vb{x}^T \vb{y} = \sum_{i=1}^n x_i y_i
\end{equation}
With vectors  $\vb{x}, \vb{y} \in \Complex^n$ we have a similarly defined inner
product defined thusly:
\begin{equation}
	\braket{\vb{x}}{\vb{y}} = \vb{x}^\dagger \vb{y} = \sum_{i=1}^n x_i^* y_i
\end{equation}

For functions/vectors in $L^2(X)$ meaning\footnote{where $X$ is most commonly
$[a,b]$ or $\Reals$} the set of functions $\{f_i\}$ that satisfy 
\[
	\int_X |f_i(x)|^2\, dx < \infty 
\]

\subsection{Coordinates and Change of Bases}

The coordinate mapping of a transformed vector $T(\ve{x})$ is given simply as

\begin{equation}
	[T(\ve{x})] = 
	\begin{bmatrix}
		[T(\ve{e}_1)]&[T(\ve{e}_2)]&\cdots&[T(\ve{e}_n)]\\
	\end{bmatrix}
	\cvb{x} = [T]\cvb{x}.
\end{equation}

In a basis $\mathcal{B}$ with basis $\{\ve{b}_i\}$, the transformation can be written

\begin{equation}
	[T(\ve{x})]_\mathcal{B} = 
	\begin{bmatrix}
		[T(\ve{b}_1)]_\mathcal{B}&[T(\ve{b}_2)]_\mathcal{B}&\cdots&[T(\ve{b}_n)]_\mathcal{B}\\
	\end{bmatrix}
	\cvb{x}_\mathcal{B} = [T]_\mathcal{B}\cvb{x}_\mathcal{B}.
\end{equation}

To switch between these two transformation matrices, we have
\begin{equation}
	[T]=B[T]_\mathcal{B}\inv{B} \iff [T]_\mathcal{B} = \inv{B}[T]B
\end{equation}

where

\begin{equation}
	B = \begin{bmatrix} 
		[\ve{b}_1]&[\ve{b}_2]& \cdots & [\ve{b}_n]\\

	\end{bmatrix}.
\end{equation}

From the above, we can see
\begin{equation}
	\cvb{x} = B\cvb{x}_\mathcal{B} \iff \cvb{x}_\mathcal{B} = \inv{B}\cvb{x}.
\end{equation}

\subsection{Diagonalization}
\begin{definitionSR}
	To \textit{diagonalize} a matrix $A$ is to express it as $A=CD\inv{C}$, where
	$C$ is an invertible matrix and $D$ is a diagonal matrix.
\end{definitionSR}
\todoin{How to diagonalize a matrix}

\begin{definitionSR}
	An \textit{orthogonal matrix} is a real matrix which has the following (mutually equivalent) properties:
	\begin{enumerate}
		\item All its columns (if treated as column vectors) are mutually orthogonal.
		\item Its transpose equals its inverse, e.g.\ if $Q$ is an orthogonal matrix, $Q^T = \inv{Q}$.
	\end{enumerate}

\end{definitionSR}

\begin{theoremSR}
	If a matrix $A$ (with real entries) is \textit{symmetric} (i.e.\ $A =
	A^T$), then it also has the following properties:
	\begin{enumerate}
		\item It is diagonalizable as $A=QDQ^T$, where $D$ is a diagonal matrix and $Q$ is a orthogonal matrix.
		\item Its basis is orthogonal.
		\item All its eigenvalues are real.
	\end{enumerate}
\end{theoremSR}

\begin{definitionSR}
	The \textit{adjoint} of a matrix $A$ is $A^\dagger = (A^T)^*$. This is pronounced ``A dagger.'' 
	In physics, 
	this is commonly known as the \textit{Hermitian conjugate} of $A$.
\end{definitionSR}
\begin{definitionSR}
	An \textit{unitary matrix} is a complex matrix which has the following (mutually equivalent) properties:
	\begin{enumerate}
		\item All its columns (if treated as column vectors) are mutually orthogonal.
		\item Its adjoint equals its inverse, e.g.\ if $Q$ is a unitary matrix, $Q^\dagger = \inv{Q}$.
	\end{enumerate}
\end{definitionSR}


\begin{theoremSR}
	If $A$ is \textit{Hermitian} (i.e.\ $A = A^\dagger$), then it also has the following properties:
	\begin{enumerate}
		\item It is diagonalizable as $A=UDU^\dagger$, where $D$ is a diagonal matrix and $U$ is a unitary matrix.
		\item Its basis is orthogonal.
		\item All its eigenvalues are real.
	\end{enumerate}
\end{theoremSR}

\section{Partial Differentiation}
\begin{definitionSR}
	A \textit{partial derivative} is the multivariate analogue of a traditional derivative.
	\[
		\pdv{f}{x_i} = \lim_{\Delta x_i \to 0} \frac{f(x_1,\cdots, x_i + \Delta x_i,
		\cdots, x_n) - f(x_1,\cdots, x_i, \cdots, x_n)}{\Delta x_i}
	\]
\end{definitionSR}

What this means in a practical context is that when taking a partial derivative, we take all other variables to be constant.
E.g.\ $f(x,y) = 3x^2\cos(y)$ 
\[
	\pdv{f}{x} = 6x\cos(y), \quad  \pdv{f}{y} = -3x^2 \sin(y)
\]

A function is \textit{differentiable} at a point if it is well approximated by
a linear function in the neighborhood of that point.

Let's say we have a function $z = f(x,y)$. We can then approximate small changes in $z$ like so:
\[
	\Delta z = f(x_0 + \Delta x, y_0 + \Delta y) - f(x_0,y_0) = \pdv{f}{x} \Delta x
	+ \pdv{f}{y} \Delta y + o(\Delta x) + o(\Delta y)
\]

Taking the limit of the above, we get the total differential
\begin{equation}
	dz = \pdv{f}{x} dx + \pdv{f}{y} dy
\end{equation}

If we have more than 2 variables of concern in a problem, e.g.\ $z = f(x,y)$
and $z = g(x, \theta)$, 
then we can use a subscript to denote what variable(s) we are holding constant,
\[
	\pdvc{z}{x}{\theta} = \pdv{g}{x}, \quad \pdvc{z}{x}{y} = \pdv{f}{x}
\]

\subsection{Implicit Partial Differentiation}
Careful manipulation of total differentials can yield partial derivatives for
complicated equations. For example, given $x^2 + y^2 + z^2 = 1$, what is
$\pdvc{z}{x}{y}$?
We can manipulate differentials as if they were algebraic quantities to
generate this derivative, 
but we must understand that ``under the hood,'' 
we are really manipulating these ``$\Delta x$'' type quantities (which are
\emph{not} symbolic in the same manner as differentials)
and then subsequently taking the \emph{limit} as described in the definition of
the partial derivative.

If we consider this equation $x^2 + y^2 + z^2 = 1$ a constant function, that
is, $F(x,y,z)=x^2 + y^2 + z^2 = 1$, then
\[
	dF = 2x\,dx + 2y\,dy + 2z\,dz
\]
Now, we are holding $y$ constant, so $\Delta y = 0$, thus we are interested in
\[
	dF_y = 2x\,dx + 2z\,dz
\]
where the subscript $y$ denotes that we are taking $y$ as constant. Since $F$
is a constant function, its differential must be zero. Thus,
\begin{align*}
	2x\,dx + 2z\,dz &= 0 \\
	2x\,dx &= -2z\,dz\\
	\pdvc{z}{x}{y} &= -\frac{x}{z} \\
\end{align*}

\subsection{The Chain Rule}
\begin{figure}[h]
	\caption{The chain rule, visualized}
	\begin{subfigure}[h]{0.3\textwidth}
		\includegraphics[width=\textwidth]{chain_rule_diagram-01}
		\caption{Interdependancies of variables in example}
	\end{subfigure}
	~~
	\begin{subfigure}[h]{0.3\textwidth}
		\includegraphics[width=\textwidth]{chain_rule_diagram-02}
		\caption{``Connections'' used when finding $\pdvc{z}{s}{t}$}
		\label{fig:chain2}
	\end{subfigure}
	~~
	\begin{subfigure}[h]{0.3\textwidth}
		\includegraphics[width=\textwidth]{chain_rule_diagram-03}
		\caption{``Connections'' used when finding $\pdvc{z}{t}{s}$}
	\end{subfigure}
\end{figure}

Just as in single variable calculus, when we differentiate compositions of two
or more functions, we have to use the chain rule.
However, the process is slightly more complicated than in the single-variable case. 
Again, we are going to utilize the \textit{total differential}.
While we \emph{can} use a formula like we do in the single-variable case,
it is fairly cumbersome, difficult to remember, and not as illuminating as the
alternative.

Suppose we have $z = z(x,y)$, $x = x(s,t)$, and $y=y(s,t)$. 
We know from earlier that
\begin{align}
	dz &= \pdv{z}{x} dx + \pdv{z}{y} dy \\
	dx &= \pdv{x}{s} ds + \pdv{x}{t} dt \\
	dy &= \pdv{y}{s} ds + \pdv{y}{t} dt 
\end{align}
Now, suppose we are interested in $\pdvc{z}{s}{t}$.
In this case, we are holding $t$ constant, and as a result, we may remove any term with $dt$.
Thus,

\begin{align}
	dz &= \pdv{z}{x} dx + \pdv{z}{y} dy \\
	dx &= \pdv{x}{s} ds  \\
	dy &= \pdv{y}{s} ds  
\end{align}

Recalling that, in truth, we are manipulating these ``$\Delta x$'' type quantites,
we may use the principle of substitution. Thus,
\begin{equation}
	dz_t = \pdv{z}{x} \pdv{x}{s} ds + \pdv{z}{y} \pdv{y}{s} ds = \pqty{\pdv{z}{x} \pdv{x}{s} + \pdv{z}{y} \pdv{y}{s} } ds
\end{equation}
If we bring the $ds$ to the other side (by utilizing the fact we are manipulating ``$\Delta x$'' type quantities), we get
\begin{equation}
	\pdvc{z}{s}{t}=  \pdv{z}{x} \pdv{x}{s} + \pdv{z}{y} \pdv{y}{s}
\end{equation}
As you can see, the chain rule can be understood as the ``chains'' of dependence between variables.
If you look at Figure~\ref{fig:chain2} on page~\pageref{fig:chain2}, you can
see the tree-like structure of the system of variables. 
Each term represents a pathway down from the function of interest to the independent variable that is not fixed.
Each line connecting two variables represents a derivative of the higher w.r.t.\ the lower.

\subsection{Optimization}
\subsubsection{Gradient}
\subsubsection{Lagrange Multipliers}

%%%%%%%%%% COMPLEX ANALYSIS %%%%%%%%%%%
\section{Complex Analysis}
\label{ComplexAnalysis}
\subsection{Complex Numbers}
\label{ComplexNumbers}
\begin{definitionSR}
	The \emph{imaginary unit}, denoted as $i$, is the
	solution\footnote{Well, not exactly. The fact is, there are two
		solutions to this equation, and we are picking one. But we could have
		just as easily picked the other! If we replaced $i$ with $-i$ in every math
	book, nothing would be incorrect (except maybe the spacing).} to the following
	equation:
	\[
		i^2 = -1
	\]
\end{definitionSR}
From this, we can generate a definition of the Complex Numbers.
\begin{definitionSR}
	The \emph{set of complex numbers}, denoted $\Complex$, can be defined as
	\[
		\Complex = \Bqty{x+iy ~ | ~ x,y \in \Reals}
	\]
\end{definitionSR}
We also define the functions $\ReP{x+iy}=x$ and $\ImP{x+iy}=y$ to work with the
real and imaginary parts more easily.
Two complex numbers are equal if and only if their corresponding real and
imaginary parts are both equal. That is,
\[
	z_1 = z_2 \iff \ReP{z_1} = \ReP{z_2} \wedge \ImP{z_1} = \ImP{z_2}.
\]
\begin{figure}[h]
	\centering
	\caption{The complex number $z$ as a Eulclidean vector.}
	\label{fig:complexAddition}
	\includegraphics[width=3in]{complex_number_diagram-01}
\end{figure}
We can think of complex numbers as analogous to Euclidean vectors with
the real and imaginary lines as the $x$ and $y$ axes. See Figure~\ref{fig:complexAddition}. 
Addition and subtraction are defined relatively simply enough:
\[
	z_1 \pm z_2 = (x_1 + i y_1) \pm (x_2 + i y_2) = (x_1 \pm x_2) + i (y_1 \pm y_2)
\]
Multiplication follows what you'd expect:

\begin{align*}
	z_1 \cdot z_2 &= (x_1 + i y_1)(x_2+ i y_2)\\
	       &= x_1 x_2 + i x_2 y_1 + i x_1 y_2 + i^2 y_1 y_2\\
	&= (x_1 x_2 - y_1 y_2) + i(x_2 y_1 + x_1 y_2)
\end{align*}

For division, what we are really asking is, 
``given that $z \neq 0$, does $\inv{z}$ exist $\forall z$ such that $z \inv{z} = 1$, and if so, what is it?'' 
Reformulating the question we get,
``given $z = x + iy \neq 0$, what is $\inv{z} = a + ib$ such that $(x+iy)(a+ib) = 1$?''
Expanding this out gives us
\[
	(xa - yb) + i(ya + xb) = 1 + 0i
\]
From the definition of equality for complex numbers that we saw earlier,
the real and imaginary parts of two complex numbers must be equal if the the
complex numbers are to be equal. Thus,
\begin{align*}
	xa - yb &= 1 \\
	ya + xb &= 0
\end{align*}
Noting that this is simply a system of linear equations, we can put it into
matrix form,
\[
	\pmqty{x & -y \\ y & x} \pmqty{a \\ b} = \pmqty{1 \\ 0}
\]
Since 
\[
	\det \pmqty{x & -y \\ y & x}  = x^2 + y^2 \neq 0
\]
the matrix is invertable, thus $a$ and $b$ exist and are unique, given by
\begin{align*}
	\pmqty{a \\ b} &= \inv{\pmqty{x & -y \\ y & x}} \pmqty{1 \\ 0} \\
		&= \frac{1}{x^2 + y^2} \pmqty{x & y \\ -y & x } \pmqty{1 \\ 0} \\
  &= \pmqty{\frac{x}{x^2+y^2} \\ \frac{-y}{x^2+y^2}} \\
\end{align*}

Thus, if $z \neq 0$, then
\[
	\inv{z} = \frac{x}{x^2+y^2} + i \frac{-y}{x^2+y^2}
\]
\begin{definitionSR}
	The \textit{complex conjugate} of a complex number $z$
	(or often times, simply the \textit{conjugate} of $z$),
	denoted\footnote{Physicists tend to use the $\conj{z}$ notation,
	while mathematicians use the $\conjugate{z}$ notation.}
	with either a star or overbar (e.g.\ $\conj{z}$ or $\conjugate{z}$),
	is the negation of $z$'s imaginary part. That is, if $z = x+iy$, then
	\[
		\conj{z} = x-iy.
	\]

\end{definitionSR}
Some important properties of the complex conjugate are as follows
(assuming $z_1, z_2 \in \Complex$):
\begin{itemize}
	\item $\conjugate{z_1+z_2} = \conjugate{z_1} + \conjugate{z_2}$
	\item $\conjugate{z_1 \cdot  z_2} = \conjugate{z_1} \cdot \conjugate{z_2}$
	\item $\conjugate{\inv{z_1}} = \inv{(\conjugate{z_1})}$

		\begin{figure}[h]
			\centering
			\caption{The complex conjugate of $z$, visualized}
			\label{fig:conjugate}
			\includegraphics[width=3in]{complex_number_diagram-03}
		\end{figure}
\end{itemize}
One can think of complex conjugation of a reflection a complex coordinate about the real axis;
see Figure~\ref{fig:conjugate}.

\begin{definitionSR}
	The \textit{absolute value} or \textit{magnitude} of a 
	complex number $z$, denoted $\abs{z}$, is the ``length'' of the Euclidean vector from the origin 
	to the point $z$ in the $\Complex$ plane. If $z=x+iy$ where $x,y \in \Reals$,
	\[
		\abs{z} = \sqrt{x^2 + y^2}
	\]
\end{definitionSR}

The absolute value of a complex number is a type of norm, specifically the
\textit{Euclidean norm} or the $L^2$ \textit{norm}. 
Some useful properties of the absolute value include:
\begin{itemize}
	\item $\abs{z_1 z_2} = \abs{z_1} \abs{z_2}$
	\item $\abs{\inv{z}} = \frac{1}{\abs{z}}$
\end{itemize}
With these definitions, we can express the inverse of a complex number as
\begin{equation}
	\inv{z} = \frac{\conj{z}}{\abs{z}^2}
\end{equation}
Or, equivalently
\begin{equation}
	z \conj{z} = \abs{z}^2
\end{equation}
\subsubsection{Polar Coordinates}
\begin{figure}[h!]
	\centering
	\caption{The complex number $z$ in polar coordinates}
	\label{fig:polar}
	\includegraphics[width=3in]{complex_number_diagram-02}
\end{figure}
We can also express complex numbers with polar coordinates;
see Figure~\ref{fig:polar}.  If we define $r=\abs{z}$
and $\theta$ as the angle from the positive real axis to the Euclidean vector
representation of $z$, then 
\begin{align}
	x &= r \cos{\theta} \\
	y &= r \sin{\theta}
\end{align}
Using Euler's formula, which I will simply state here as\footnote{If you
haven't seen this before, go derive it by Taylor expanding $e^{i\theta}, \sin
\theta, \cos \theta$.} 
\[
	e^{i \theta} = \cos{\theta} + i \sin{\theta},
\]
we can also express the complex number $z$ as $z=r e^{i \theta}$.
Euler's formula has many powerful results that are quite simple to derive. One
of the most trivial is De Moive's Theorem.
\begin{theoremSR}[De Moive's Theorem]
	\begin{align*}
		e^{in\theta} &= (\cos{\theta} + i \sin{\theta})^n \\
		\cos{n\theta} + i \sin{n\theta} &= (\cos{\theta} + i \sin{\theta})^n
	\end{align*}
\end{theoremSR}
This can be used to easily generate useful identites such as 
\[
	\cos{3\theta} = \cos^3{\theta} - 3\cos{\theta}\sin^2{\theta}
\]
This $re^{i\theta}$ notation helps to elucidate the geometry of mathematical
operations in $\Complex$: 
\[
	z_1=r_1 e^{i\theta_1}, z_2=r_2 e^{i\theta_2}
	\Rightarrow z_1 z_2 = r_1 r_2 e^{i(\theta_1 + \theta_2)},
\]
 or in other words,
when multiplying two complex numbers, we multiply their magnitudes and 
add their angles from $\Re$ to get the complete product. Thus, multiplying
a complex number by $e^{i\theta}$ will simply rotate it (while preserving its
length).\footnote{Note that if $in\theta = i\theta \mod 2\pi$, repeated
rotations can loop back upon themselves (e.g.\ $\pqty{e^{i\frac{\pi}{2}}}^4 z =
z$), while if $in\theta \neq i\theta \mod 2\pi$, $\pqty{e^{i\theta}}^n z\neq z$
for $n\neq0$. (The previous assumes $n\in\mathbb{Z}$).} Additionally,
\[z=re^{i\theta} \Rightarrow \inv{z} = re^{-i\theta},\]
or in other words, when inverting complex numbers, we simply negate the angle
from $\Re$, which makes perfect sense considering that $\inv{z}$ is just a
scaled $\conj{z}$. 
\label{argument}
You might note that $\theta$ (known as the \textit{argument} of $z$)
is not unique for a given $z$. For example,
the points (expressed as $(r,\theta)$ coordinate pairs)
$(2, \frac{\pi}{4})$ and $(2, \frac{9\pi}{4})$ refer to the same point in 
$\Complex$. Thus, if we tried to create a function $\arg{z}$ such that
if $z=re^{i \theta}$, $\arg{z} = \theta$,
we would fail, as $\arg{z} = \theta + 2 \pi$ 
is also perfectly valid.
However, we \emph{can} define $\arg{z}$ as a ``\textit{multivalued}
function.''\todo{add notes about Riemann surfaces}
Thus, for a given $z=re^{i\theta}$, $\arg{z} = \theta + 2n\pi$
(where $n \in \Integers$).
All of these different values of $\arg z$ differ by multiples of $2\pi i$, 
each belonging to different \textit{branches} of the $\arg z$.
If we pick just one of those branches of $\arg{z}$, say,
the branch with the range $[0, 2\pi)$, 
it becomes an honest-to-god \emph{function}, which we denote as
$\Arg{z}$.\footnote{This
is very similar to the definition of $\arcsin \theta$.}
This branch is so commonly used, it is known as the 
\textit{principal branch}.\footnote{Note that another extremely common branch
	to denote as the ``principal branch'' is the one with the branch
$(-\pi,\pi]$. The decision of where to make this \textit{branch cut} is 
completely arbitrary and does not affect any of the mathematics.}

The functions $\arg{z}$ and $\Arg{z}$ have the following properties:
\begin{itemize}
	\item $\arg{z_1 z_2} = \arg{z_1} + \arg{z_2}$
	\item $\arg{\inv{z}} = - \arg{z}$
\end{itemize}
which fits with our geometric understanding of $\Complex$ that we developed 
earlier.

Roots and fractional powers are not too much more compicated when dealing with 
complex numbers. Just as the root of a real number may have more
than one answer (e.g.$\sqrt{64} = 8 \text{or} -8$), so too may the root of a 
complex number be multivalued. If $z = re^{i\theta}$,
\begin{align*}
	z^\frac{n}{m} = &r^\frac{n}{m} e^{i\frac{n\theta}{m}},\\ 
					&r^\frac{n}{m} e^{i\frac{n\theta}{m} + \frac{2\pi}{m}},\\ 
					&r^\frac{n}{m}e^{i\frac{n\theta}{m} + \frac{2\pi(2)}{m}},\\ 
					&\ldots,\\
					&r^\frac{n}{m} e^{i\frac{n\theta}{m} + \frac{2\pi(m-1)}{m}}
\end{align*}
Thus, there are $m$ distinct values for the term $z^\frac{n}{m}$.\footnote{Assuming
rational powers, i.e.\ $m,n \in \mathbb{N}$.}
\subsection{Complex Series, $e^z$, and the Logarithm}
\subsubsection{Complex Series}
Overall, complex series behave similarly to their real counterparts.
Let's define the infinite complex series $S$ as 
\begin{equation}
	\sum_{n=1}^{\infty} a_n = S \qquad (a_n \in \Complex)
\end{equation}
If $a_n = x_n + i y_n$ (where $x_n, y_n \in \Reals$), consider
\[
	X_N = \sum_{n=1}^{N} x_n, \qquad Y_N = \sum_{n=1}^{N} y_n.
\]
Let's then define
\[
	X = \lim_{N \to \infty} X_N, \qquad Y = \lim_{N \to \infty} Y_N.
\]
Thus, $S$ exists (i.e.\ the series converges) iff $X$ and $Y$ exist.

The concept of absolute convergence also applies to complex series.
That is, if $\sum \abs{a_n}$ converges, then $ \sum a_n$ converges absolutely. 
One can derive this from the fact that $\abs{x_n} \leq \abs{a_n} \wedge
\abs{y_n} \leq \abs{a_n}$.

Complex power series also behave similarly to real power series. The generic
complex power series about a point $a$ can be defined as
\begin{equation}
	S(z) = \sum_{n=1}^{\infty} a_n (z - a) \qquad (a, a_n \in \Complex)
\end{equation}
There is some $R$ such that for
\begin{align*}
	\abs{z-a} &< R, \quad S(z)~\text{converges} \\
	\abs{z-a} &> R, \quad S(z)~\text{diverges} \\
\end{align*}
This power series will converge within this \textit{disk of convergence} and
diverge outside of it.

Note that we can still use some tests to determine convergence. The most common
test to use in the complex case is the ratio test, which looks identical to the
ratio test described earlier.

For example, let us examine the \textit{complex exponential function}, i.e.\
$e^z$. Now, from Taylor series, we know that we can express $e^x$ as
\[
	e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \ldots.
\]
This also holds true for complex arguments. That is, for $z \in \Complex$,
\begin{equation}
	e^z = 1 + z + \frac{z^2}{2!} + \frac{z^3}{3!} + \ldots.
\end{equation}
But does this converge everywhere (as in the real case)? Let us use the ratio
test.
\begin{align*}
	\rho &= \lim_{n \to \infty} \abs{\frac{a_{n+1}}{a_n}} \\
      &= \lim_{n \to \infty} \abs{\frac{z^{n+1}}{(n+1)!} \cdot \frac{n!}{z^n}} \\
      &= \lim_{n \to \infty} \abs{\frac{z}{n+1}} \\
      &= 0
\end{align*}
Thus, the series converges regardless of the value of $z$, 
and therefore, converges everywhere on $\Complex$.
\subsubsection{The Complex Exponential, $e^z$}
For the task of visualizing $e^z$, we are stuck with a problem. We are mapping
a 2D space to another 2D space, and unfortunately, we humans cannot visualize
the 4D space.\footnote{Elon Musk, I'm looking at you to come up with a solution
to this.} This issue will arise with any complex to complex functions and thus,
we must be clever with how we plot them.
\begin{figure}
	\centering
	\begin{subfigure}[b]{.4\textwidth}
		\includegraphics[width=\textwidth]{exponential_Re}
		\caption{$\ReP{e^{x+iy}}$}
	\end{subfigure}
	\begin{subfigure}[b]{.4\textwidth}
		\includegraphics[width=\textwidth]{exponential_Im}
		\caption{$\ImP{e^{x+iy}}$}
	\end{subfigure}
	\caption{The function $e^z$.}
	\label{fig:Re_Im_exp}
\end{figure}
First, we may to plot the real and imaginary parts, like in
Figure~\ref{fig:Re_Im_exp}. However, this does not yield a great deal of
new insight.
\begin{figure}
	\centering
	\begin{subfigure}[b]{.4\textwidth}
		\begin{tikzpicture}[baseline=(origin.base)]
		\node (origin) at (0,0) {};
		\draw [color=black!50, arrows={<->[length=3]}] (0,-3) -- (0,3) node [above] {$\Im$};
		\draw [color=black!50, arrows={<->[length=3]}] (-2,0) -- (2,0) node [right] {$\Re$}; 
		\draw [color=black,dotted,very thick] (0,-2.9) -- (0,2.9);
		\draw [color=black] (-1.9,.6) node [above right] {$z=x+it$} -- (1.9,.6); 
		\draw [color=black] (-1.9,1.6) node [above right] {$z=x+i(t+2\pi)$} -- (1.9,1.6); 
		\draw [color=black, dashed] (1,-2.9) node [above right] {$z=s+iy$} -- (1,2.9); 
		\draw [color=black,very thick] (-1.9,0) -- (1.9,0);
		\end{tikzpicture}
		\caption{Input of $e^z$}
	\end{subfigure}
	\qquad
	\begin{subfigure}[b]{.4\textwidth}
		\begin{tikzpicture}[baseline=(origin.base)]
		\node (origin) at (0,0) {};
		\draw [color=black!50, arrows={<->[length=3]}] (0,-3) -- (0,3) node [above] {$\Im$};
		\draw [color=black!50, arrows={<->[length=3]}] (-2,0) -- (2,0) node [right] {$\Re$}; 
		\draw [color=black,very thick] (0,0) -- (1.9,0);
		\draw [black,dotted,very thick] (0,0) circle (.6);
		\filldraw [red!50] (.6,0) circle (1.5pt) node [below right] {1};
		\draw [black,dashed] (0,0) circle (1.5);
		\filldraw [red!50] (1.5,0) circle (1.5pt) node [below right] {$e^s$};
		\draw [color=black, rotate around={60:(0,0)}] (0,0) -- (3.22,0);
		\draw [color=black,arrows=->] (1,0) arc (0:60:1) node [midway, right] {$t$};
		\end{tikzpicture}
		\caption{Output of $e^z$}
	\end{subfigure}
	\caption{Each curve in the left plot represents a subset of $\Complex$ which
		maps to a corresponding subset in the right plot. Note that $x$ and $y$
	may vary, while $s$ and $t$ are (real) fixed values.}
	\label{fig:exp_C_to_C}
\end{figure}
Another approach we may take is restrict ourselves to important 1D subsets of  
$\Complex$ and see where those map to via the function $e^z$. See
Figure~\ref{fig:exp_C_to_C} for this visualization approach. Note that $e^z$ is
very much \emph{not} one-to-one: for every horizontal line, all other horizontal
lines $2\pi in$ above and below\footnote{where $n\in \mathbb{Z}$} map to the
same values---i.e.\ $e^{x+iy} = e^{x+i(y+2\pi n)}$. This is very different from
real function $e^x$. 
\subsubsection {The Complex Logarithm}

For the inverse of the exponential function, the
logarithm, we will generate a multi-valued ``function.'' An actual inverse
will require us to restrict our attention to a subset to the domain of $e^z$.

Let us play with the properties of both of these inverses. The multi-valued 
logarithm\footnote{We will denote the \emph{natural} logarithm as $\log z$.},
denoted $z = \log w$ (where $w,z \in \Complex$), is the value such 
that $e^z = w$. From the properties of the complex exponential, we find 
\[
	\log(w_1) + \log(w_2) = \log(w_1 w_2)
\]
which is the same property had by the real logarithm.

If we work in polar coordinates, we
get the following form:
\begin{align*}
	\log\pqty{re^{i\theta}} &= \log(r) + \log\pqty{e^{i\theta}}
	\log\pqty{re^{i\theta}} &= \log(r) + i(\theta + 2\pi k),
\end{align*}
where $k\in \mathbb{Z}$. This leads us to a definition:
\begin{definitionSR}
	If $z\neq0$ and $z = re^{i\theta}$
	\[
		\log z = \log re^{i\theta} = \log r + \arg w.
	\]
\end{definitionSR}
Just as $\arg z$ is multivalued, so too is $\log z$. Making the same branch
cut as we did to generate $\Arg z$ from $\arg z$, we get the \emph{function} of
$\Log z$.
\begin{definitionSR}
	If $z\neq0$ and $z = re^{i\theta}$
	\[
		\Log z = \Log re^{i\theta} = \log r + \Arg z.
	\]
\end{definitionSR}

The logirithm allows us to evaluate numbers raised to complex powers, which we'll
see, may be multivalued just in the same manner as in the case of
\hyperref[argument]{the argument} of a complex number.

Recall that $a^b = e^{b \log a}$ (where $a,b\in\Complex$). The evaluation of 
this expression is best done as series of examples:
\begin{example}
	Evaluate $i^{-i}$.\\
	\textit{Solution}.
	\begin{align*}
		i^{-i} &= e^{i\log(-i)}\\
				&= e^{i \cdot i\qty(\frac{3\pi}{2} + 2\pi k)}\\
				&= e^{-\pqty{\frac{3\pi}{2} + 2\pi k}}
	\end{align*}
	where $k\in\Integers$. Note that the expression takes on an
	\emph{infinite} number of values!
\end{example}
\begin{example}
	Evaluate $ \pqty{i-1}^{i+1}$.\\
	\textit{Solution}.
	\begin{align*}
		\pqty{i-1}^{i+1} &= e^{(i+1)\log(i-1)}\\
		  &= e^{(i+1)\qty(\log{\frac{1}{\sqrt{2}}} + i\qty(\frac{3\pi}{4} + 2\pi k))}\\
		  &= e^{\pqty{\frac{1}{2}\log{2} -\qty(\frac{3\pi}{4} + 2\pi k)}}
		e^{i\pqty{\frac{1}{2}\log{2} + \frac{3\pi}{4} + 2\pi k}}
	\end{align*}
	where $k \in \Integers$. Note that the expression also takes on an
	\emph{infinite} number of values!
\end{example}
\subsection{Complex Differentiation}
For the purposes of this section, let us assume $f : \Complex \to \Complex$.
Recall that the \textit{differentiation} is the approximation of a function by a
linear function at a point.
\begin{definitionSR}
	The \textit{derivative} of $f$ at $z$ is
	\begin{equation*}
		f'(z) = \lim_{\Delta z \to 0} \frac{f(z + \Delta z) -
		f(z)}{\Delta z}
	\end{equation*}
	where $\Delta z \equiv \Delta x + i \Delta y$.
\end{definitionSR}
The above definition implies that
\begin{equation}
	f(z + \Delta z) = f(z) + f'(z) \Delta z + o(\Delta z)
\end{equation}
The usual rules apply to complex differentiation:
\begin{itemize}
	\item $(f + g)' = f' + g'$
	\item $(fg)' = f'g + fg'$ 
	\item $f(g(z))' = f'(g(z))g'(z)$ (assuming derivatives exist)
\end{itemize}
Many derivatives take forms similar to their real counterparts, e.g.\ $(z^2)' =
2z$, $(e^z)' = e^z$. 
\begin{figure}
	\centering
	\begin{tikzpicture}
	\coordinate (a) at (1.3,1.2);
	\filldraw [color=black] (a) circle (1pt) node [below left] {$z$};
	\draw [color=black, arrows={->[length=3]}] (0,0) -- (0,3) node [above] {$\Im$};
	\draw [color=black, arrows={->[length=3]}] (0,0) -- (4,0) node [right] {$\Re$}; 
	\draw [color=black, arrows={->[length=3]}] ($ (a) + (0,2) $) node [right] {Vertical} -- ($(a) + (0,.1)$);
	\draw [color=black, arrows={->[length=3]}] ($ (a) + (2,0) $) node [above] {Horizontal} -- ($(a) + (.1,0)$);
	\end{tikzpicture}
	\caption{Different paths of approach towards a point $z$ in taking the limit
	$\lim_{\Delta z \to 0}$. Note that these are just 2 approaches out
	of an infinite number of possibilities.}
\end{figure}
However, not all functions are differentiable. 

When we defined the limit included in the definition of the complex derivative,
we appeared to treat
the difference $\Delta z$  identically to the difference $\Delta x$ of the
limit in the definition of the real derivative.
However, while with real limits, we are only navigating the real number line,
i.e.\ there are only two directions: positive or negative. With $\Delta z$, we
are working with the complex plane: there are many ways for $\Delta z$ to
diminish to zero, as there are many ways one can approach the point $z$ at
which we are differentiating. If following different paths approach different
values, the limit doesn't exist---just like how real limits don't exist if the
left hand and right hand limits don't agree. 

For example, consider the
function $f(z) = \conjugate{z}$, equivalent to the map $x+iy \mapsto x-iy$:
\[
	\lim_{\Delta z \to 0} \frac{\conjugate{z + \Delta z} -
	\conjugate{z}}{\Delta z} = \lim_{\Delta z \to 0}
	\frac{\conjugate{\Delta z}}{\Delta z}
\]

Let us now go through two different approaches towards the point $z$:
\begin{itemize}
	\item $\Delta z \to 0$ horizontally: $\Delta z = \Delta x$, $\Delta x \to 0$
		\[
			\lim_{\Delta x \to 0} \frac{\conjugate{\Delta x}}{\Delta x} = 
			\lim_{\Delta x \to 0} \frac{\Delta x}{\Delta x} = \lim_{\Delta x \to 0} 1 = 1
		\] 
	\item $\Delta z \to 0$ vertically: $\Delta z = i\Delta y$, $\Delta y \to 0$
		\[
			\lim_{\Delta y \to 0} \frac{\conjugate{i \Delta y}}{i \Delta y} = 
			\lim_{\Delta y \to 0} \frac{- i\Delta y}{i\Delta y} = \lim_{\Delta y \to 0}-1 = -1
		\]
\end{itemize}
These results are not equal, therefore the limit does not exist. Thus,
$f(z)=\conjugate{z}$ is not differentiable.
But why is that the case? What is it about the function
$f(z)=\conjugate{z}$ that makes it different?

To look at this from another angle\footnote{Thanks are
	owed to the responses
\href{http://math.stackexchange.com/questions/180849/why-is-the-complex-number-z-abi-equivalent-to-the-matrix-form-left-begins}{here}.},
consider the function
$F$, equivalent to the map
\[
	\bmqty{x \\ y} \mapsto \bmqty{x \\  -y} 
\] 
This transformation can be written as the matrix
\[
	F = \bmqty{1 & 0 \\ 0 & -1}
\]
as
\[
	F\pqty{\bmqty{x \\ y} }= \bmqty{1 & 0 \\ 0 & -1} \bmqty{x \\ y}  =
	\bmqty{x \\ -y}.
\]
Multiplication by a complex number $a + bi$ is equivalent
to multiplication by a matrix
\[
	\bmqty{a & -b \\ b & a}
\]
as
\[
	(a + bi) (x + iy) = (ax - by) + i(bx + ay) \longleftrightarrow
	\bmqty{ax - by \\ bx + ay} = \bmqty{a & -b \\ b & a} \bmqty{x \\ y}.
\]
But as we showed above, the transformation $F$ doesn't take this form.
Therefore, it cannot be expressed as a complex number. This means that we
cannot express a first order approximation of $f$ by expanding the function in a 
series about a complex number $z_0$ like
\begin{align*}
	f(z_0+\Delta z) &= f(z_0) + f'(z_0)\Delta z + \cdots \\	
	\conjugate{z_0 + \Delta z} &= \conjugate{z_0} + f'(z_0)\Delta z + \cdots
\end{align*}
\subsubsection{Cauchy-Riemann Equations}
Suppose $f:\Complex \to \Complex$, $f(x+iy) = u(x,y) + i v(x,y)$, 
and $f$ is differentiable at $z=x+iy$. 

By these assumptions, the limit
\[
\lim_{\Delta z \to 0} \frac{f(z+\Delta z) - f(z)}{\Delta z}
\] exists for any approach towards $z$ and its value is 
independent of that approach.
If we examine the horizontal and vertical approaches,\todo{Include proof} 
we derive the Cauchy-Riemann equations:
\begin{align}
\pdv{u}{x} &= \pdv{v}{y}\\
\pdv{v}{x} &= -\pdv{u}{y}.
\end{align}
Thus, satisfying the equations above is a \emph{necessary} condition for
differentiability.  As it turns out, satisfying the Cauchy-Riemann equations
is also a \emph{sufficient} condition for differentiability.

A side-effect of the Cauchy-Riemann equations is that both the functions $u$ and $v$
must satisfy Laplace's equation:
\begin{align}
\pdv[2]{u}{x} + \pdv[2]{u}{y} &= 0\\
\pdv[2]{v}{x} + \pdv[2]{v}{y} &= 0
\end{align}

Differentiability generally depends upon the domain we are inspecting.
For functions of a real variable, we generally discuss intervals.
For functions of a \emph{complex} variable, we instead use \textit{disks}.
\begin{definitionSR}
An \textit{open disk} is a subset of the complex plane
\[
\Bqty{z: (z-a)<r} \subseteq \Complex
\]
for some $a,r$. By contrast, a \textit{closed disk} would be
\[
\Bqty{z: (z-a)\leq r} \subseteq \Complex.
\]
\end{definitionSR}
\begin{figure}[h]
\centering
\begin{subfigure}[b]{.4\textwidth}
\begin{tikzpicture}
\coordinate (a) at (2.3,1.2);
\filldraw [black,dashed, fill=black!10] (a) circle (1);
%\draw [color=black, arrows={->[length=2]}] (0,0) -- (a) node [right] {$a$}; 
\filldraw [color=black] (a) circle (1pt) node [right] {$a$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (0,3) node [above] {$\Im$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (4,0) node [right] {$\Re$}; 
\draw [color=black, rotate around={20:(a)}] (a) -- ($ (a) + (0,1) $) node [midway,right] {$r$};
\end{tikzpicture}
\caption{Open disk}
\end{subfigure}
~~
\begin{subfigure}[b]{.4\textwidth}
\begin{tikzpicture}
\coordinate (a) at (2.3,1.2);
\filldraw [black, fill=black!10] (a) circle (1);
%\draw [color=black, arrows={->[length=2]}] (0,0) -- (a) node [right] {$a$}; 
\filldraw [color=black] (a) circle (1pt) node [right] {$a$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (0,3) node [above] {$\Im$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (4,0) node [right] {$\Re$}; 
\draw [color=black, rotate around={20:(a)}] (a) -- ($ (a) + (0,1) $) node [midway,right] {$r$};
\end{tikzpicture}
\caption{Closed disk}
\end{subfigure}
\end{figure}
\todoin{Define a region}
\begin{theoremSR}
If $f$ is differentiable in a disk containing $z$,
\begin{enumerate}
\item $f$ is infinitely differentiable.
\item $f(z+\Delta z) = \sum_{n=0}^\infty \frac{f^{(n)}(z)}{n!} (\Delta z)^n$
converges absolutely in the disk, that is, the function is equal to its Taylor
series, and thus, is ``analytic''.
\end{enumerate}
\end{theoremSR}
The above theorem demonstrates that differentiable functions are incredibly
well behaved, and that this ``good behavior'' is a local property.
\subsection{Contour Integration}
For functions $f:\Reals \to \Reals$ and $f:\Reals \to \Complex$ we integrate
over intervals (e.g.\ $[a,b]$). For functions $f:\Complex \to \Complex$, we
integrate over \textit{curves}.
\begin{definitionSR}
A \textit{curve} is a differentiable function $\gamma : [a,b] \to \Complex$
where $\gamma'(t) \neq 0$. If $\gamma$ doesn't intersect
itself\footnote{intersections would be cases where $\gamma(t_1) = \gamma(t_2)$
for $t_1\neq t_2$} (except possibly at endpoints), then the curve is \textit{simple}.

Integration along $\gamma$ is simply
\[
\int_\gamma f(z)\dd{z} = \int_a^b f(\gamma(t))\gamma'(t)\dd{t},
\]
noting that $\dd{z} = \gamma'(t)\dd{t}$.
\end{definitionSR}
See Figure~\ref{fig:curves} for examples of curves.
\begin{figure}
\centering
\begin{subfigure}[b]{.4\textwidth}
\begin{tikzpicture}
\coordinate (a) at (2.3,1.2);
\draw[
decoration={markings, mark=at position 0.625 with {\arrow{>}}},
postaction={decorate}]
 (a) circle (1);
%\draw [color=black, arrows={->[length=2]}] (0,0) -- (a) node [right] {$a$}; 
\node at ($ (a) - (1,0) $) [label=left:$\gamma$] {};
\filldraw [color=black] (a) circle (1pt) node [right] {$a$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (0,3) node [above] {$\Im$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (4,0) node [right] {$\Re$}; 
\draw [color=black, rotate around={20:(a)}] (a) -- ($ (a) + (0,1) $) node [midway,right] {$r$};

\end{tikzpicture}
\caption{$\gamma(t) = a + re^{it}$, $(t \in [0,2\pi])$}
\end{subfigure}
~~
\begin{subfigure}[b]{.4\textwidth}
\begin{tikzpicture}
\coordinate (z1) at (.5,1);
\coordinate (z2) at (3.8,2.5);
\draw [->-=.5] (z1) -- (z2) node [midway, below] {$\gamma$};
\filldraw [color=black] (z1) circle (1pt) node [above] {$z_1$};
\filldraw [color=black] (z2) circle (1pt) node [right] {$z_2$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (0,3) node [above] {$\Im$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (4,0) node [right] {$\Re$}; 
\end{tikzpicture}
\caption{$\gamma(t) = (1-t)z_1 + tz_2$, $(t \in [0,1])$}
\end{subfigure}
\caption{Examples of curves}
\label{fig:curves}
\end{figure}
\begin{definitionSR}
A \textit{contour} is a concatenation of a finite number 
of simple curves meeting at their endpoints. If it does not intersect 
itself, it is a \textit{simple contour}.
\end{definitionSR}
\begin{definitionSR}
	A simple closed\footnote{That is, it ends where it starts.} contour is 
	\textit{positively oriented} if the interior\footnote{The \textit{interior} 
	is the bounded region defined by the contour.} is to the \emph{left} of the 
	contour's direction (denoted by the arrow). Otherwise, the contour is 
	\textit{negatively oriented}.
\end{definitionSR}
See Figure~\ref{fig:contours} for examples of contours.
\begin{figure}[h]
\centering
\begin{subfigure}[b]{.4\textwidth}
\begin{tikzpicture}
\coordinate (z1) at (.5,1);
\coordinate (z2) at (2.1,.3);
\coordinate (z3) at (3.8,2.5);
\draw [->-=.5] (z1) to [bend left] node [midway, below] {$\gamma_1$} (z2);
\draw [->-=.5] (z2) to [bend left] node [midway, right] {$\gamma_2$} (z3);
\filldraw [color=black] (z1) circle (1pt);
\filldraw [color=black] (z2) circle (1pt);
\filldraw [color=black] (z3) circle (1pt);
\draw [color=black, arrows={->[length=3]}] (0,0) -- (0,3) node [above] {$\Im$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (4,0) node [right] {$\Re$}; 
\end{tikzpicture}
\caption{Example of simple contour $C=\gamma_1 + \gamma_2$}
\end{subfigure}
\begin{subfigure}[b]{.4\textwidth}
\begin{tikzpicture}
\coordinate (z1) at (.5,1);
\coordinate (z2) at (2.1,.3);
\coordinate (z3) at (3.8,2.5);
\draw [->-=.5] (z1) to [bend right] node [midway, above] {$\gamma_3$} (z2);
\draw [->-=.5] (z2) to [bend right] node [midway, right] {$\gamma_2$} (z3);
\draw [->-=.5] (z3) to [bend right] node [midway, below] {$\gamma_1$} (z1);
\filldraw [color=black] (z1) circle (1pt);
\filldraw [color=black] (z2) circle (1pt);
\filldraw [color=black] (z3) circle (1pt);
\draw [color=black, arrows={->[length=3]}] (0,0) -- (0,3) node [above] {$\Im$};
\draw [color=black, arrows={->[length=3]}] (0,0) -- (4,0) node [right] {$\Re$}; 
\end{tikzpicture}
\caption{Example of a simple, closed, positively oriented contour $C=\gamma_1 + \gamma_2 + \gamma_3$}
\end{subfigure}
\caption{Examples of contours}
\label{fig:contours}
\end{figure}
\begin{theoremSR}[Cauchy-Goursat Theorem]
	If $f$ is analytic \emph{on} and \emph{inside} a simple closed closed
	contour $C$, then
	\[
		\oint_C f(z)\dd{z} = 0.
	\]
\end{theoremSR}
In general, to prove Cauchy-Goursat, one may use
\begin{proof}\\
	\todoin{Proof of Cauchy-Goursat}
\end{proof}

\begin{figure}
	\centering
	\begin{tikzpicture}
		\filldraw [color=white!0, light fill] plot[smooth cycle, tension=.7]
			coordinates {(.2,.2)  (4,.6)  (4.2,3)  (.3,2) };
		\coordinate (z1) at (1,.5);
		\coordinate (z2) at (3.8,2.5);
		\node at (.6,1.4) {$R$};
		\draw [->-=.5] (z1) to [bend left] node [midway, above] {$C_1$} (z2);
		\draw [->-=.5] (z1) to [bend right] node [midway, below] {$C_2$} (z2);
		\filldraw [color=black] (z1) circle (1pt) node [left] {$z_1$};
		\filldraw [color=black] (z2) circle (1pt) node [right] {$z_2$};
		\draw [color=black, arrows={->[length=3]}] (0,0) -- (0,3) node [above] {$\Im$};
		\draw [color=black, arrows={->[length=3]}] (0,0) -- (4,0) node [right] {$\Re$}; 
	\end{tikzpicture}
	\caption{$\int_{z_1}^{z_2} f(z)\dd{z}$ is the same, regardless of which path
	we take.}
	\label{fig:pathind}
\end{figure}
One consequence of the Cauchy-Goursat theorem is that if $f$ is analytic
on a \textit{simply connected}  region $R$ (that is, the region has no holes)
and $z_1, z_2 \in R$, then 
\[
	\int_{C_1} f(z)\dd{z} = \int_{C_2} f(z)\dd{z}
\]
for any $C_1, C_2$ from $z_1$ to $z_2$. See Figure~\ref{fig:pathind} for a 
visualization.
\begin{proof}
	Consider $C=C_1 - C_2$. $C$ is a closed simple contour; $f$ is analytic 
	on $C$ and inside $C$.
	\[
		\int_{C_1} f(z)\dd{z} - \int_{C_2} f(z) \dd{z} = \oint_C f(z) \dd{z} = 0,
	\]
	by Cauchy-Goursat. Thus,	
	\[
		\int_{C_1} f(z)\dd{z} = \int_{C_2} f(z) \dd{z} 
	\]
\end{proof}
\subsubsection{Cauchy Integral Formula}
What about regions where a function is not analytic? 
\begin{example}
	Evaluate the contour integral
	\[
		\oint_C \frac{1}{z}\dd{z} 
	\]
	where $C$ is a positively oriented circle of radius $\rho$. \\
	\textit{Solution}.
	We note that we cannot use the Cauchy-Goursat theorem as $\frac{1}{z}$ is
	not analytic at $z=0$. The simplest parametrization of the curve $C$ is
	\[
	\gamma(t) = \rho e^{it} \Rightarrow \gamma'(t) = \rho i e^{it}.
	\]
	From this, it's fairly easy to evaluate the integral	
	\begin{align*}
		\oint_C \frac{1}{z}\dd{z} &= \int_0^{2\pi} \frac{1}{\rho e^{it}} \rho i e^{it} \dd{t}\\
								&= i \int_0^{2\pi} \dd{t}\\
								&= 2\pi i.
	\end{align*}
\end{example}
	Note that this value is completely independent of the radius $\rho$, so the
	value of the integral is the same in the limit $\rho \to 0$. This will 
	prove very important in just a moment. 

	\begin{theoremSR}[Cauchy Integral Formula]
		If $f$ is analytic on and inside a simple closed positively oriented
		contour $C$, then for any $a \in R$, where $R$ is the interior of the 
		curve, then
		\[
			f(a) = \frac{1}{2\pi i} \oint \frac{f(z)}{z-a} \dd{z}.
		\]
	\end{theoremSR}
	\begin{figure}
		\centering
		\begin{tikzpicture}
		\coordinate (a) at (2.25,1.1);
		\filldraw [color=black] (a) circle (1pt) node [left] {$a$};
		\draw[decoration={markings, mark=between positions 0.2 and 0.7 step .5 with {\arrow{>}}},
		postaction={decorate}] (2.2,1.7) arc (93:-258:.6);
		\draw[decoration={markings, mark=between positions 0.2 and 0.7 step .5 with {\arrow{>}}},
		postaction={decorate}] 
		plot[smooth, tension=.4]
		coordinates {(1.7,2.5) (.3,1.9) (.2,.2)  (3.5,.6) (4.2,3) (1.8,2.52) };
		\draw[->-=.3] (1.8,2.52) -- (2.2,1.7) node [midway, right] {$A$}; 
		\draw[->-=.3] (2.1,1.68) -- (1.7,2.5) node [midway, left] {$B$}; 
		\coordinate (z1) at (1,.5);
		\coordinate (z2) at (3.8,2.5);
		\node at (1.2,1.1) {$-C'$};
		\node at (4.6,3) {$C$};
		\end{tikzpicture}
		\caption{The contour $\Gamma$ referenced by the proof.}
		\label{fig:cauchy_integral_formula}
	\end{figure}
	\begin{proof}
		The closed contour $\Gamma$ is depicted in
		Figure~\ref{fig:cauchy_integral_formula}, and is a sum of curves:
		\[
			\Gamma = C + A - C' + B
		\]
		where $-C$ refers to the curve $C'$ but oppositely oriented.
		Suppose $f$ is a function analytic on and inside $\Gamma$. By
		the Cauchy-Goursat Theorem,\footnote{as $a$ is on the exterior of $\Gamma$.}
		\begin{align*}
			\oint\limits_\Gamma \frac{f(z)}{z-a} \dd{z} &= 0\\
			\int\limits_C \frac{f(z)}{z-a} \dd{z} +  \int\limits_A \frac{f(z)}{z-a} \dd{z} 
			+ \int\limits_{-C'} \frac{f(z)}{z-a} \dd{z} + \int\limits_B \frac{f(z)}{z-a} \dd{z} &= 0\\
			\int\limits_C \frac{f(z)}{z-a} \dd{z} +  \int\limits_A \frac{f(z)}{z-a} \dd{z} 
			- \int\limits_{C'} \frac{f(z)}{z-a} \dd{z} + \int\limits_B \frac{f(z)}{z-a} \dd{z} &= 0.
		\end{align*}
		Now, if we make the cut smaller, we find that the curve $A$ approaches
		$-B$:
		\[
			\int\limits_A \frac{f(z)}{z-a} \dd{z} + \int\limits_B
			\frac{f(z)}{z-a} \dd{z} \to \int\limits_A \frac{f(z)}{z-a} \dd{z} -
			\int\limits_A \frac{f(z)}{z-a} \dd{z} = 0
		\]
		So,
		\begin{align*}
			&\int\limits_C \frac{f(z)}{z-a} \dd{z} - \int\limits_{C'}
			\frac{f(z)}{z-a} \dd{z} \to 0\\
			&\int\limits_C \frac{f(z)}{z-a} \dd{z} = \int\limits_{C'} \frac{f(z)}{z-a}
		\end{align*}
		What the contour integral $I_\rho$ around $C'$? The simplest
		parametrization of the curve is $\gamma(t) = a + \rho e^{it}\,
		\pqty{t\in[0,2\pi]}$.  Using this parametrization, 
		\[
			I_\rho = \int_0^{2\pi} \frac{f\qty(a + \rho e^{it})}{a+e^{it}-a}
			\rho i e^{it} \dd{t} = i \int_0^{2\pi} f\pqty{a + \rho e^{it}}
			\dd{t}
		\]
		In the limit $\rho \to 0$, 
		\begin{align*}
			\lim_{\rho \to 0} I_\rho &= \lim_{\rho \to 0} i \int_0^{2\pi} f\pqty{a + \rho e^{it}} \dd{t} \\
									 &= i \int_0^{2\pi} f(a) \dd{t} \\
									 &= 2\pi i f(a)
		\end{align*}
		Thus,
		\begin{align*}
			2\pi i f(a) &= \oint\limits_C \frac{f(z)}{z-a} \dd{z}\\
			f(a) &= \frac{1}{2\pi i}\oint\limits_C \frac{f(z)}{z-a} \dd{z}
		\end{align*}
	\end{proof}
	The Cauchy Integral Formula has several significant consequences. Firstly,
	it shows that for analytic functions, the value at any given point is
	determined by the value at (not necessarily closely) surrounding points. 
	It also puts conditions on a function's differentiability nearby a point
	where it is analytic.
	\begin{theoremSR}
		If $f$ is analytic at $z$, then it's differentiable in a disk containing
		$z$.
	\end{theoremSR}
	\begin{proof}
		Writing the Cauchy Integral Formula with a simple swap of variables, we 
		have
		\[
			f(z) = \frac{1}{2\pi i} \oint \frac{f(w)}{w-z} \dd{w}
		\]
		\todoin{Finish Proof}
	\end{proof}

\subsection{Residue Calculus}


\section{Harmonic Analysis}
\label{HarmonicAnalysis}
\subsection{Fourier Series}
\subsection{The Fourier Transform}
\subsection{The Laplace Transform}
\end{document}
